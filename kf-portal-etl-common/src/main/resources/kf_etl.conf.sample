io.kf.etl {

  spark {
    app.name = ""
    master = ""
  }

  postgresql {
    host = ""
    database = ""
    user = ""
    password = ""
  }

  elasticsearch {
    url = ""
    index = ""
  }

  hdfs {
    defaultFS = "hdfs://"
    root = ""
  }

  processors = [
    {
      name = "download"
      postgresql = ${io.kf.etl.postgresql}
      hpo {
//        postgresql = ${io.kf.etl.postgresql}
        mysql = {
          host = ""
          database = ""
          user = ""
          password = ""
        }
      }
      dump_path = ${io.kf.etl.hdfs.root}/pg_dump
    },
    {
      name = "document"
      /**
       **  each processor has a "data_path" field which defines a full URL path for the current processor to store its data.
       **  this field is optional, if not present, the default value will be ${io.kf.etl.hdfs.root}/${processor.name}
       **  A user could use this field to control where the data would be written
       **/
      data_path = ${io.kf.etl.hdfs.root}/document
      write_intermediate_data = false
    },
    {
      name = "index"
      elasticsearch = ${io.kf.etl.elasticsearch}
    }
  ]

}